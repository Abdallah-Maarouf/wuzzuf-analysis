{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Power BI Data Preparation and Optimization\n",
                "\n",
                "This notebook demonstrates the Power BI data preparation process, including data optimization, relationship documentation, and validation procedures.\n",
                "\n",
                "## Objectives:\n",
                "1. Optimize CSV files for Power BI performance\n",
                "2. Create data model documentation\n",
                "3. Prepare aggregated datasets to improve dashboard responsiveness\n",
                "4. Validate data relationships for proper Power BI modeling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"üìö Libraries imported successfully\")\n",
                "print(f\"üìÅ Working directory: {Path.cwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Optimization Results\n",
                "\n",
                "Review the optimized data files created for Power BI import."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check optimized files\n",
                "print(\"üìä Power BI Optimized Files:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "processed_dir = Path('data/processed')\n",
                "powerbi_files = list(processed_dir.glob('*_powerbi.csv'))\n",
                "\n",
                "total_size = 0\n",
                "for file in sorted(powerbi_files):\n",
                "    size_mb = file.stat().st_size / (1024 * 1024)\n",
                "    total_size += size_mb\n",
                "    \n",
                "    # Load file to get row count\n",
                "    try:\n",
                "        df = pd.read_csv(file)\n",
                "        rows = len(df)\n",
                "        cols = len(df.columns)\n",
                "        print(f\"‚úÖ {file.name}\")\n",
                "        print(f\"   üìè Size: {size_mb:.2f} MB\")\n",
                "        print(f\"   üìä Dimensions: {rows:,} rows √ó {cols} columns\")\n",
                "        print()\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error reading {file.name}: {e}\")\n",
                "\n",
                "print(f\"üì¶ Total optimized data size: {total_size:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Model Validation\n",
                "\n",
                "Validate the relationships and data integrity for Power BI import."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load main tables for validation\n",
                "print(\"üîç Data Model Validation:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "try:\n",
                "    # Load optimized tables\n",
                "    jobs_df = pd.read_csv('data/processed/jobs_powerbi.csv')\n",
                "    skills_df = pd.read_csv('data/processed/skills_powerbi.csv')\n",
                "    job_skills_df = pd.read_csv('data/processed/job_skills_powerbi.csv')\n",
                "    \n",
                "    print(\"üìã Table Dimensions:\")\n",
                "    print(f\"   Jobs (Fact Table): {len(jobs_df):,} rows √ó {len(jobs_df.columns)} columns\")\n",
                "    print(f\"   Skills (Dimension): {len(skills_df):,} rows √ó {len(skills_df.columns)} columns\")\n",
                "    print(f\"   Job-Skills (Bridge): {len(job_skills_df):,} rows √ó {len(job_skills_df.columns)} columns\")\n",
                "    \n",
                "    print(\"\\nüîó Relationship Validation:\")\n",
                "    \n",
                "    # Check job_id relationships\n",
                "    jobs_ids = set(jobs_df['job_id'].unique())\n",
                "    job_skills_job_ids = set(job_skills_df['job_id'].unique())\n",
                "    \n",
                "    jobs_with_skills = len(jobs_ids.intersection(job_skills_job_ids))\n",
                "    jobs_without_skills = len(jobs_ids - job_skills_job_ids)\n",
                "    \n",
                "    print(f\"   ‚úÖ Jobs with skills: {jobs_with_skills:,} ({jobs_with_skills/len(jobs_df)*100:.1f}%)\")\n",
                "    print(f\"   ‚ö†Ô∏è  Jobs without skills: {jobs_without_skills:,} ({jobs_without_skills/len(jobs_df)*100:.1f}%)\")\n",
                "    \n",
                "    # Check skill_id relationships\n",
                "    skills_ids = set(skills_df['skill_id'].unique())\n",
                "    job_skills_skill_ids = set(job_skills_df['skill_id'].unique())\n",
                "    \n",
                "    skills_used = len(skills_ids.intersection(job_skills_skill_ids))\n",
                "    skills_unused = len(skills_ids - job_skills_skill_ids)\n",
                "    \n",
                "    print(f\"   ‚úÖ Skills used in jobs: {skills_used:,} ({skills_used/len(skills_df)*100:.1f}%)\")\n",
                "    print(f\"   ‚ö†Ô∏è  Skills not used: {skills_unused:,} ({skills_unused/len(skills_df)*100:.1f}%)\")\n",
                "    \n",
                "    print(\"\\nüìä Data Quality Summary:\")\n",
                "    print(f\"   ‚Ä¢ Total unique jobs: {len(jobs_ids):,}\")\n",
                "    print(f\"   ‚Ä¢ Total unique skills: {len(skills_ids):,}\")\n",
                "    print(f\"   ‚Ä¢ Total job-skill relationships: {len(job_skills_df):,}\")\n",
                "    print(f\"   ‚Ä¢ Average skills per job: {len(job_skills_df)/len(jobs_df):.1f}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error during validation: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pre-Aggregated Summary Tables\n",
                "\n",
                "Review the pre-aggregated tables created for dashboard performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Review summary tables\n",
                "print(\"üìà Pre-Aggregated Summary Tables:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "summary_tables = {\n",
                "    'skills_summary_powerbi.csv': 'Top skills by demand',\n",
                "    'monthly_trends_powerbi.csv': 'Monthly posting trends',\n",
                "    'experience_summary_powerbi.csv': 'Experience level distribution',\n",
                "    'location_summary_powerbi.csv': 'Geographic distribution',\n",
                "    'industry_summary_powerbi.csv': 'Industry breakdown'\n",
                "}\n",
                "\n",
                "for filename, description in summary_tables.items():\n",
                "    try:\n",
                "        df = pd.read_csv(f'data/processed/{filename}')\n",
                "        print(f\"\\nüìä {filename}\")\n",
                "        print(f\"   üìù Description: {description}\")\n",
                "        print(f\"   üìè Dimensions: {len(df):,} rows √ó {len(df.columns)} columns\")\n",
                "        print(f\"   üî§ Columns: {', '.join(df.columns.tolist())}\")\n",
                "        \n",
                "        # Show top 3 rows as preview\n",
                "        print(f\"   üëÄ Preview (top 3 rows):\")\n",
                "        for i, row in df.head(3).iterrows():\n",
                "            print(f\"      {i+1}. {dict(row)}\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error reading {filename}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Type Optimization Analysis\n",
                "\n",
                "Analyze the data type optimizations applied for Power BI performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze data types in optimized files\n",
                "print(\"üîß Data Type Optimization Analysis:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Jobs table data types\n",
                "print(\"\\nüìã Jobs Table Data Types:\")\n",
                "jobs_dtypes = jobs_df.dtypes\n",
                "for col, dtype in jobs_dtypes.items():\n",
                "    print(f\"   {col}: {dtype}\")\n",
                "\n",
                "# Check for categorical columns\n",
                "categorical_cols = [col for col, dtype in jobs_dtypes.items() if dtype.name == 'category']\n",
                "print(f\"\\nüè∑Ô∏è  Categorical columns ({len(categorical_cols)}): {', '.join(categorical_cols)}\")\n",
                "\n",
                "# Check for datetime columns\n",
                "datetime_cols = [col for col, dtype in jobs_dtypes.items() if 'datetime' in str(dtype)]\n",
                "print(f\"üìÖ DateTime columns ({len(datetime_cols)}): {', '.join(datetime_cols)}\")\n",
                "\n",
                "# Check for numeric columns\n",
                "numeric_cols = [col for col, dtype in jobs_dtypes.items() if pd.api.types.is_numeric_dtype(dtype)]\n",
                "print(f\"üî¢ Numeric columns ({len(numeric_cols)}): {', '.join(numeric_cols)}\")\n",
                "\n",
                "# Memory usage analysis\n",
                "print(\"\\nüíæ Memory Usage Analysis:\")\n",
                "memory_usage = jobs_df.memory_usage(deep=True)\n",
                "total_memory_mb = memory_usage.sum() / (1024 * 1024)\n",
                "print(f\"   Jobs table memory usage: {total_memory_mb:.2f} MB\")\n",
                "\n",
                "# Show top memory consuming columns\n",
                "top_memory_cols = memory_usage.nlargest(5)\n",
                "print(\"   Top 5 memory consuming columns:\")\n",
                "for col, usage in top_memory_cols.items():\n",
                "    usage_mb = usage / (1024 * 1024)\n",
                "    print(f\"      {col}: {usage_mb:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Power BI Import Readiness Check\n",
                "\n",
                "Final validation to ensure data is ready for Power BI import."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Power BI readiness checklist\n",
                "print(\"‚úÖ Power BI Import Readiness Check:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "readiness_checks = []\n",
                "\n",
                "# Check 1: All required files exist\n",
                "required_files = ['jobs_powerbi.csv', 'skills_powerbi.csv', 'job_skills_powerbi.csv']\n",
                "files_exist = all((Path('data/processed') / file).exists() for file in required_files)\n",
                "readiness_checks.append((\"All required files exist\", files_exist))\n",
                "\n",
                "# Check 2: No null primary keys\n",
                "jobs_null_ids = jobs_df['job_id'].isnull().sum()\n",
                "skills_null_ids = skills_df['skill_id'].isnull().sum()\n",
                "no_null_keys = (jobs_null_ids == 0) and (skills_null_ids == 0)\n",
                "readiness_checks.append((\"No null primary keys\", no_null_keys))\n",
                "\n",
                "# Check 3: Reasonable file sizes (not too large for Power BI)\n",
                "jobs_size_mb = (Path('data/processed/jobs_powerbi.csv').stat().st_size) / (1024 * 1024)\n",
                "reasonable_size = jobs_size_mb < 100  # Less than 100MB\n",
                "readiness_checks.append((f\"Reasonable file size ({jobs_size_mb:.1f}MB < 100MB)\", reasonable_size))\n",
                "\n",
                "# Check 4: Relationships are valid\n",
                "valid_relationships = (jobs_with_skills > 0) and (skills_used > 0)\n",
                "readiness_checks.append((\"Valid table relationships\", valid_relationships))\n",
                "\n",
                "# Check 5: Documentation exists\n",
                "docs_exist = (Path('powerbi/data_model_documentation.md').exists() and \n",
                "              Path('powerbi/import_validation_checklist.md').exists())\n",
                "readiness_checks.append((\"Documentation files exist\", docs_exist))\n",
                "\n",
                "# Display results\n",
                "all_passed = True\n",
                "for check_name, passed in readiness_checks:\n",
                "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
                "    print(f\"   {status} {check_name}\")\n",
                "    if not passed:\n",
                "        all_passed = False\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "if all_passed:\n",
                "    print(\"üéâ POWER BI IMPORT READY!\")\n",
                "    print(\"‚úÖ All readiness checks passed\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  POWER BI IMPORT NEEDS ATTENTION\")\n",
                "    print(\"‚ùå Some readiness checks failed\")\n",
                "\n",
                "print(\"\\nüìã Next Steps:\")\n",
                "print(\"   1. Open Power BI Desktop\")\n",
                "print(\"   2. Import data/processed/*_powerbi.csv files\")\n",
                "print(\"   3. Follow powerbi/import_validation_checklist.md\")\n",
                "print(\"   4. Establish relationships as documented\")\n",
                "print(\"   5. Create dashboard visualizations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary: Power BI Data Preparation Complete\n",
                "\n",
                "Review the complete Power BI data preparation results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üìä POWER BI DATA PREPARATION SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "print(\"\\n‚úÖ Optimizations Applied:\")\n",
                "optimizations = [\n",
                "    \"Data type optimization for better performance\",\n",
                "    \"Categorical encoding for text fields\",\n",
                "    \"Date parsing and validation\",\n",
                "    \"Calculated columns for enhanced analysis\",\n",
                "    \"Memory usage optimization\",\n",
                "    \"File size optimization for import\"\n",
                "]\n",
                "\n",
                "for opt in optimizations:\n",
                "    print(f\"   ‚Ä¢ {opt}\")\n",
                "\n",
                "print(\"\\nüìÅ Files Created:\")\n",
                "created_files = [\n",
                "    \"jobs_powerbi.csv - Main fact table (optimized)\",\n",
                "    \"skills_powerbi.csv - Skills dimension table\",\n",
                "    \"job_skills_powerbi.csv - Relationship bridge table\",\n",
                "    \"skills_summary_powerbi.csv - Pre-aggregated skills data\",\n",
                "    \"monthly_trends_powerbi.csv - Pre-aggregated time trends\",\n",
                "    \"experience_summary_powerbi.csv - Pre-aggregated experience data\",\n",
                "    \"location_summary_powerbi.csv - Pre-aggregated location data\",\n",
                "    \"industry_summary_powerbi.csv - Pre-aggregated industry data\"\n",
                "]\n",
                "\n",
                "for file in created_files:\n",
                "    print(f\"   ‚úì {file}\")\n",
                "\n",
                "print(\"\\nüìã Documentation Created:\")\n",
                "docs = [\n",
                "    \"data_model_documentation.md - Complete data model guide\",\n",
                "    \"import_validation_checklist.md - Step-by-step import guide\"\n",
                "]\n",
                "\n",
                "for doc in docs:\n",
                "    print(f\"   ‚úì {doc}\")\n",
                "\n",
                "print(\"\\nüéØ Power BI Dashboard Requirements Met:\")\n",
                "requirements = [\n",
                "    \"Optimized CSV files for Power BI import ‚úÖ\",\n",
                "    \"Data model documentation ‚úÖ\",\n",
                "    \"Pre-aggregated datasets for performance ‚úÖ\",\n",
                "    \"Data relationship validation ‚úÖ\",\n",
                "    \"Import validation checklist ‚úÖ\"\n",
                "]\n",
                "\n",
                "for req in requirements:\n",
                "    print(f\"   {req}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"üöÄ TASK 5.2 COMPLETED SUCCESSFULLY!\")\n",
                "print(\"Ready for Power BI dashboard creation!\")\n",
                "print(\"=\" * 60)"
            ]
        }
    ],\n",
    \"metadata\": {\n",
        \"kernelspec\": {\n",
            \"display_name\": \"Python 3\",\n",
            \"language\": \"python\",\n",
            \"name\": \"python3\"\n",
},\n",
        \"language_info\": {\n",
            \"codemirror_mode\": {\n",
                \"name\": \"ipython\",\n",
                \"version\": 3\n",
},\n",
            \"file_extension\": \".py\",\n",
            \"mimetype\": \"text/x-python\",\n",
            \"name\": \"python\",\n",
            \"nbconvert_exporter\": \"python\",\n",
            \"pygments_lexer\": \"ipython3\",\n",
            \"version\": \"3.8.5\"\n",
}\n",
},\n",
    \"nbformat\": 4,\n",
    \"nbformat_minor\": 4\n}